{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOG3rDJoN7StSeOgVpeCqPx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hyojgun/hyojeong/blob/main/LSTM_tem.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HZfIHibSqBXB",
        "outputId": "0df186ff-9950-4029-fdc3-82289b8e7bd3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import string\n",
        "\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/data/ArticlesApril2017.csv\")\n",
        "print(df.columns)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SPBV5tN-qB2q",
        "outputId": "944e8f1b-0038-45ee-edfc-dd77312eab49"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index(['abstract', 'articleID', 'articleWordCount', 'byline', 'documentType',\n",
            "       'headline', 'keywords', 'multimedia', 'newDesk', 'printPage', 'pubDate',\n",
            "       'sectionName', 'snippet', 'source', 'typeOfMaterial', 'webURL'],\n",
            "      dtype='object')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import glob\n",
        "import pandas as pd\n",
        "import string\n",
        "from torch.utils.data.dataset import Dataset\n",
        "\n",
        "class TextGeneration(Dataset):\n",
        "    def clean_text(self, txt):\n",
        "        \"\"\"\n",
        "        모든 단어를 소문자로 변환하고, 텍스트에서 구두점을 제거합니다.\n",
        "\n",
        "        Args:\n",
        "            txt (str): 원본 텍스트 문자열.\n",
        "\n",
        "        Returns:\n",
        "            str: 소문자로 변환되고 구두점이 제거된 텍스트.\n",
        "        \"\"\"\n",
        "        txt = \"\".join(v for v in txt if v not in string.punctuation).lower()\n",
        "        return txt\n",
        "\n",
        "    def __init__(self):\n",
        "        \"\"\"\n",
        "        TextGeneration 데이터셋 초기화 함수입니다.\n",
        "\n",
        "        - 지정된 CSV 파일에서 모든 헤드라인을 불러옵니다.\n",
        "        - 'Unknown'으로 표시된 헤드라인을 제거합니다.\n",
        "        - 텍스트 전처리를 통해 모든 헤드라인을 정리하여 코퍼스를 만듭니다.\n",
        "        - 단어를 고유한 인덱스로 지정하는 bag-of-words (BOW) 딕셔너리를 생성합니다.\n",
        "        - 모델 학습에 사용할 입력 시퀀스를 생성합니다.\n",
        "        \"\"\"\n",
        "        all_headlines = []\n",
        "\n",
        "        # 'Articles'가 포함된 파일에서 모든 텍스트 불러오기\n",
        "        for filename in glob.glob(\"/content/drive/MyDrive/Colab Notebooks/data/*.csv\"):\n",
        "            if 'Articles' in filename:\n",
        "                article_df = pd.read_csv(filename)\n",
        "\n",
        "                # 데이터프레임에서 'headline' 열의 값을 리스트에 추가\n",
        "                all_headlines.extend(list(article_df.headline.values))\n",
        "                break\n",
        "\n",
        "        # 'Unknown' 값 제거\n",
        "        all_headlines = [h for h in all_headlines if h != \"Unknown\"]\n",
        "\n",
        "        # 구두점 제거 및 전처리된 문장 리스트 생성\n",
        "        self.corpus = [self.clean_text(x) for x in all_headlines]\n",
        "        self.BOW = {}\n",
        "\n",
        "        # 모든 문장의 단어를 추출하여 고유번호를 지정\n",
        "        for line in self.corpus:\n",
        "            for word in line.split():\n",
        "                if word not in self.BOW.keys():\n",
        "                    self.BOW[word] = len(self.BOW.keys())\n",
        "\n",
        "        # 모델 입력으로 사용할 데이터 생성\n",
        "        self.data = self.generate_sequence(self.corpus)\n",
        "\n",
        "    def generate_sequence(self, txt):\n",
        "        \"\"\"\n",
        "        단어 시퀀스를 생성하여 입력-정답 데이터 쌍을 만듭니다.\n",
        "\n",
        "        Args:\n",
        "            txt (list of str): 코퍼스의 문장 리스트.\n",
        "\n",
        "        Returns:\n",
        "            list of tuple: 입력 시퀀스와 해당 정답 단어 쌍 리스트.\n",
        "        \"\"\"\n",
        "        seq = []\n",
        "\n",
        "        for line in txt:\n",
        "            line = line.split()\n",
        "            line_bow = [self.BOW[word] for word in line]\n",
        "\n",
        "            # 단어 2개를 입력, 다음 단어를 정답으로 설정\n",
        "            data = [([line_bow[i], line_bow[i+1]], line_bow[i+2])\n",
        "                    for i in range(len(line_bow) - 2)]\n",
        "\n",
        "            seq.extend(data)\n",
        "\n",
        "        return seq\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"\n",
        "        데이터셋의 총 길이를 반환합니다.\n",
        "\n",
        "        Returns:\n",
        "            int: 데이터셋의 총 길이.\n",
        "        \"\"\"\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        \"\"\"\n",
        "        인덱스 i에 해당하는 데이터와 레이블을 반환합니다.\n",
        "\n",
        "        Args:\n",
        "            i (int): 데이터 인덱스.\n",
        "\n",
        "        Returns:\n",
        "            tuple: 입력 데이터와 정답 레이블.\n",
        "            - 입력 데이터 (np.array): 모델의 입력으로 사용할 단어 시퀀스.\n",
        "            - 정답 레이블 (np.array): 모델의 출력으로 사용할 다음 단어.\n",
        "        \"\"\"\n",
        "        data = np.array(self.data[i][0])  # 입력 데이터\n",
        "        label = np.array(self.data[i][1]).astype(np.float32)  # 출력 데이터\n",
        "\n",
        "        return data, label\n"
      ],
      "metadata": {
        "id": "S2xBvACBqL9n"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class LSTM(nn.Module):\n",
        "    def __init__(self, num_embeddings):\n",
        "        \"\"\"\n",
        "        LSTM 네트워크 초기화 함수입니다.\n",
        "\n",
        "        Args:\n",
        "            num_embeddings (int): 단어 사전의 크기 (임베딩할 단어 개수).\n",
        "        \"\"\"\n",
        "        super(LSTM, self).__init__()\n",
        "\n",
        "        # 임베딩 층: 단어를 고차원의 희소 벡터에서 밀집된 벡터로 변환하여 학습의 효율성을 높입니다.\n",
        "        # 예를 들어, num_embeddings가 10,000이고 embedding_dim이 16이라면, 각 단어를 16차원의 벡터로 표현\n",
        "        self.embed = nn.Embedding(\n",
        "            num_embeddings=num_embeddings,  # 단어 사전 크기\n",
        "            embedding_dim=16                # 각 단어의 임베딩 벡터 차원\n",
        "        )\n",
        "\n",
        "        # LSTM 층: 임베딩 벡터를 입력받아 시계열 패턴을 학습합니다.\n",
        "        # input_size는 임베딩 차원과 동일하게 설정하며, hidden_size는 LSTM 셀의 차원 크기입니다.\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=16,                  # LSTM 입력 차원 (임베딩 벡터 차원과 일치)\n",
        "            hidden_size=64,                 # LSTM 숨김층의 차원\n",
        "            num_layers=5,                   # LSTM 층의 개수 (깊이)\n",
        "            batch_first=True                # 배치 차원이 첫 번째 위치\n",
        "        )\n",
        "\n",
        "        # 완전 연결층 (fc1): LSTM 출력 차원을 단어 사전 크기에 맞추기 위해 변환합니다.\n",
        "        # 여기서는 hidden_size가 64이고, 두 개의 LSTM 출력을 연결해 입력 크기는 128로 설정\n",
        "        self.fc1 = nn.Linear(128, num_embeddings)\n",
        "\n",
        "        # 완전 연결층 (fc2): 최종 예측값을 생성\n",
        "        self.fc2 = nn.Linear(num_embeddings, num_embeddings)\n",
        "\n",
        "        # ReLU 활성화 함수: 비선형성을 추가해 학습 능력을 향상시킵니다.\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        순전파 함수로, 입력 x에 대해 예측값을 계산합니다.\n",
        "\n",
        "        Args:\n",
        "            x (torch.Tensor): 입력 데이터 (문장 속 단어의 인덱스 시퀀스)\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: 모델의 예측 결과 (단어 사전 크기와 동일한 차원으로 출력)\n",
        "        \"\"\"\n",
        "        # 임베딩 층을 통해 단어 인덱스를 밀집 벡터로 변환합니다.\n",
        "        x = self.embed(x)\n",
        "\n",
        "        # LSTM 층을 통과하며 시계열 정보를 학습합니다.\n",
        "        x, _ = self.lstm(x)\n",
        "\n",
        "        # LSTM의 출력을 2차원으로 변형하여 완전 연결층에 맞게 변환\n",
        "        x = torch.reshape(x, (x.shape[0], -1))\n",
        "\n",
        "        # 첫 번째 완전 연결층을 거쳐 예측 벡터 생성\n",
        "        x = self.fc1(x)\n",
        "\n",
        "        # 활성화 함수 적용\n",
        "        x = self.relu(x)\n",
        "\n",
        "        # 두 번째 완전 연결층을 거쳐 최종 출력\n",
        "        x = self.fc2(x)\n",
        "\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "neIPzbWvm7Zf"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tqdm\n",
        "\n",
        "from torch.utils.data.dataloader import DataLoader\n",
        "from torch.optim.adam import Adam\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "dataset = TextGeneration()\n",
        "model = LSTM(num_embeddings=len(dataset.BOW)).to(device)\n",
        "loader = DataLoader(dataset, batch_size=64)\n",
        "optim = Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "for epoch in range(250):\n",
        "  iterator =tqdm.tqdm(loader)\n",
        "  for data, label in iterator:\n",
        "    optim.zero_grad()\n",
        "\n",
        "    pred = model(torch.tensor(data, dtype= torch.long).to(device))\n",
        "\n",
        "    loss = nn.CrossEntropyLoss()(\n",
        "        pred, torch.tensor(label, dtype= torch.long).to(device))\n",
        "\n",
        "    loss.backward()\n",
        "    optim.step()\n",
        "\n",
        "    iterator.set_description(f\"epoch:{epoch+1} loss:{loss.item()}\")\n",
        "torch.save(model.state_dict(), \"lstm.pth\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M265-sPTn45H",
        "outputId": "32d8e8fd-33f6-4498-f88e-d3dc8a3c84f1"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/104 [00:00<?, ?it/s]<ipython-input-10-6257ab505ae7>:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  pred = model(torch.tensor(data, dtype= torch.long).to(device))\n",
            "<ipython-input-10-6257ab505ae7>:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  pred, torch.tensor(label, dtype= torch.long).to(device))\n",
            "epoch:1 loss:7.415966987609863: 100%|██████████| 104/104 [00:01<00:00, 89.45it/s]\n",
            "epoch:2 loss:6.923974514007568: 100%|██████████| 104/104 [00:01<00:00, 89.59it/s]\n",
            "epoch:3 loss:6.337023735046387: 100%|██████████| 104/104 [00:01<00:00, 88.48it/s]\n",
            "epoch:4 loss:5.80476713180542: 100%|██████████| 104/104 [00:01<00:00, 90.10it/s]\n",
            "epoch:5 loss:5.407581806182861: 100%|██████████| 104/104 [00:01<00:00, 90.97it/s]\n",
            "epoch:6 loss:5.3897318840026855: 100%|██████████| 104/104 [00:01<00:00, 91.42it/s]\n",
            "epoch:7 loss:5.158023834228516: 100%|██████████| 104/104 [00:01<00:00, 91.68it/s]\n",
            "epoch:8 loss:5.206933975219727: 100%|██████████| 104/104 [00:01<00:00, 91.52it/s]\n",
            "epoch:9 loss:5.47351598739624: 100%|██████████| 104/104 [00:01<00:00, 91.00it/s]\n",
            "epoch:10 loss:6.110394477844238: 100%|██████████| 104/104 [00:01<00:00, 90.93it/s]\n",
            "epoch:11 loss:6.845939636230469: 100%|██████████| 104/104 [00:01<00:00, 90.26it/s]\n",
            "epoch:12 loss:6.893970966339111: 100%|██████████| 104/104 [00:01<00:00, 89.83it/s]\n",
            "epoch:13 loss:6.815577983856201: 100%|██████████| 104/104 [00:01<00:00, 87.49it/s]\n",
            "epoch:14 loss:6.762233257293701: 100%|██████████| 104/104 [00:01<00:00, 88.57it/s]\n",
            "epoch:15 loss:6.534968376159668: 100%|██████████| 104/104 [00:01<00:00, 91.17it/s]\n",
            "epoch:16 loss:6.431085109710693: 100%|██████████| 104/104 [00:01<00:00, 89.37it/s]\n",
            "epoch:17 loss:6.359677314758301: 100%|██████████| 104/104 [00:01<00:00, 90.56it/s]\n",
            "epoch:18 loss:6.294790744781494: 100%|██████████| 104/104 [00:01<00:00, 90.20it/s]\n",
            "epoch:19 loss:6.238284111022949: 100%|██████████| 104/104 [00:01<00:00, 91.04it/s]\n",
            "epoch:20 loss:6.188194751739502: 100%|██████████| 104/104 [00:01<00:00, 90.80it/s]\n",
            "epoch:21 loss:6.141639709472656: 100%|██████████| 104/104 [00:01<00:00, 90.56it/s]\n",
            "epoch:22 loss:6.08180046081543: 100%|██████████| 104/104 [00:01<00:00, 90.59it/s]\n",
            "epoch:23 loss:6.025564193725586: 100%|██████████| 104/104 [00:01<00:00, 87.55it/s]\n",
            "epoch:24 loss:5.959386825561523: 100%|██████████| 104/104 [00:01<00:00, 88.40it/s]\n",
            "epoch:25 loss:5.875101089477539: 100%|██████████| 104/104 [00:01<00:00, 90.42it/s]\n",
            "epoch:26 loss:5.910403251647949: 100%|██████████| 104/104 [00:01<00:00, 90.27it/s]\n",
            "epoch:27 loss:5.9131975173950195: 100%|██████████| 104/104 [00:01<00:00, 90.73it/s]\n",
            "epoch:28 loss:5.713511943817139: 100%|██████████| 104/104 [00:01<00:00, 90.33it/s]\n",
            "epoch:29 loss:5.9780073165893555: 100%|██████████| 104/104 [00:01<00:00, 89.88it/s]\n",
            "epoch:30 loss:5.869327068328857: 100%|██████████| 104/104 [00:01<00:00, 90.22it/s]\n",
            "epoch:31 loss:5.729066848754883: 100%|██████████| 104/104 [00:01<00:00, 90.65it/s]\n",
            "epoch:32 loss:5.655912399291992: 100%|██████████| 104/104 [00:01<00:00, 90.43it/s]\n",
            "epoch:33 loss:5.5854878425598145: 100%|██████████| 104/104 [00:01<00:00, 87.97it/s]\n",
            "epoch:34 loss:5.511532783508301: 100%|██████████| 104/104 [00:01<00:00, 86.03it/s]\n",
            "epoch:35 loss:5.440125465393066: 100%|██████████| 104/104 [00:01<00:00, 88.56it/s]\n",
            "epoch:36 loss:5.394739151000977: 100%|██████████| 104/104 [00:01<00:00, 90.51it/s]\n",
            "epoch:37 loss:5.336639881134033: 100%|██████████| 104/104 [00:01<00:00, 90.60it/s]\n",
            "epoch:38 loss:5.285074234008789: 100%|██████████| 104/104 [00:01<00:00, 90.70it/s]\n",
            "epoch:39 loss:5.173730850219727: 100%|██████████| 104/104 [00:01<00:00, 91.11it/s]\n",
            "epoch:40 loss:5.106351852416992: 100%|██████████| 104/104 [00:01<00:00, 90.05it/s]\n",
            "epoch:41 loss:5.090977191925049: 100%|██████████| 104/104 [00:01<00:00, 91.22it/s]\n",
            "epoch:42 loss:5.103518962860107: 100%|██████████| 104/104 [00:01<00:00, 90.80it/s]\n",
            "epoch:43 loss:5.180586814880371: 100%|██████████| 104/104 [00:01<00:00, 89.95it/s]\n",
            "epoch:44 loss:5.08870792388916: 100%|██████████| 104/104 [00:01<00:00, 88.49it/s]\n",
            "epoch:45 loss:4.96461296081543: 100%|██████████| 104/104 [00:01<00:00, 87.05it/s]\n",
            "epoch:46 loss:4.942922592163086: 100%|██████████| 104/104 [00:01<00:00, 90.42it/s]\n",
            "epoch:47 loss:4.87897253036499: 100%|██████████| 104/104 [00:01<00:00, 90.58it/s]\n",
            "epoch:48 loss:4.767322540283203: 100%|██████████| 104/104 [00:01<00:00, 90.43it/s]\n",
            "epoch:49 loss:4.67927885055542: 100%|██████████| 104/104 [00:01<00:00, 89.81it/s]\n",
            "epoch:50 loss:4.831361770629883: 100%|██████████| 104/104 [00:01<00:00, 90.72it/s]\n",
            "epoch:51 loss:4.635076522827148: 100%|██████████| 104/104 [00:01<00:00, 90.77it/s]\n",
            "epoch:52 loss:4.719008922576904: 100%|██████████| 104/104 [00:01<00:00, 90.04it/s]\n",
            "epoch:53 loss:4.7236785888671875: 100%|██████████| 104/104 [00:01<00:00, 91.17it/s]\n",
            "epoch:54 loss:4.426475524902344: 100%|██████████| 104/104 [00:01<00:00, 88.66it/s]\n",
            "epoch:55 loss:4.426058769226074: 100%|██████████| 104/104 [00:01<00:00, 84.87it/s]\n",
            "epoch:56 loss:4.359472274780273: 100%|██████████| 104/104 [00:01<00:00, 89.62it/s]\n",
            "epoch:57 loss:4.388784408569336: 100%|██████████| 104/104 [00:01<00:00, 90.14it/s]\n",
            "epoch:58 loss:4.268723011016846: 100%|██████████| 104/104 [00:01<00:00, 89.73it/s]\n",
            "epoch:59 loss:4.0698065757751465: 100%|██████████| 104/104 [00:01<00:00, 91.32it/s]\n",
            "epoch:60 loss:4.014101028442383: 100%|██████████| 104/104 [00:01<00:00, 90.75it/s]\n",
            "epoch:61 loss:3.8446576595306396: 100%|██████████| 104/104 [00:01<00:00, 90.09it/s]\n",
            "epoch:62 loss:3.699388027191162: 100%|██████████| 104/104 [00:01<00:00, 90.62it/s]\n",
            "epoch:63 loss:3.698835611343384: 100%|██████████| 104/104 [00:01<00:00, 90.99it/s]\n",
            "epoch:64 loss:3.6702146530151367: 100%|██████████| 104/104 [00:01<00:00, 90.14it/s]\n",
            "epoch:65 loss:3.655038356781006: 100%|██████████| 104/104 [00:01<00:00, 86.17it/s]\n",
            "epoch:66 loss:3.577777862548828: 100%|██████████| 104/104 [00:01<00:00, 87.50it/s]\n",
            "epoch:67 loss:3.6955926418304443: 100%|██████████| 104/104 [00:01<00:00, 90.61it/s]\n",
            "epoch:68 loss:3.8008782863616943: 100%|██████████| 104/104 [00:01<00:00, 90.41it/s]\n",
            "epoch:69 loss:3.6750290393829346: 100%|██████████| 104/104 [00:01<00:00, 90.95it/s]\n",
            "epoch:70 loss:3.5320794582366943: 100%|██████████| 104/104 [00:01<00:00, 90.12it/s]\n",
            "epoch:71 loss:3.4741933345794678: 100%|██████████| 104/104 [00:01<00:00, 90.45it/s]\n",
            "epoch:72 loss:3.3335931301116943: 100%|██████████| 104/104 [00:01<00:00, 89.44it/s]\n",
            "epoch:73 loss:3.2165215015411377: 100%|██████████| 104/104 [00:01<00:00, 89.86it/s]\n",
            "epoch:74 loss:3.0666275024414062: 100%|██████████| 104/104 [00:01<00:00, 90.00it/s]\n",
            "epoch:75 loss:3.1255805492401123: 100%|██████████| 104/104 [00:01<00:00, 90.25it/s]\n",
            "epoch:76 loss:3.1771790981292725: 100%|██████████| 104/104 [00:01<00:00, 86.58it/s]\n",
            "epoch:77 loss:3.2190184593200684: 100%|██████████| 104/104 [00:01<00:00, 90.48it/s]\n",
            "epoch:78 loss:2.884775400161743: 100%|██████████| 104/104 [00:01<00:00, 90.49it/s]\n",
            "epoch:79 loss:2.7431933879852295: 100%|██████████| 104/104 [00:01<00:00, 90.65it/s]\n",
            "epoch:80 loss:2.7939181327819824: 100%|██████████| 104/104 [00:01<00:00, 90.35it/s]\n",
            "epoch:81 loss:2.7396421432495117: 100%|██████████| 104/104 [00:01<00:00, 89.88it/s]\n",
            "epoch:82 loss:2.733429193496704: 100%|██████████| 104/104 [00:01<00:00, 91.11it/s]\n",
            "epoch:83 loss:2.754944086074829: 100%|██████████| 104/104 [00:01<00:00, 90.63it/s]\n",
            "epoch:84 loss:2.7239551544189453: 100%|██████████| 104/104 [00:01<00:00, 90.48it/s]\n",
            "epoch:85 loss:2.579007148742676: 100%|██████████| 104/104 [00:01<00:00, 89.35it/s]\n",
            "epoch:86 loss:2.7377023696899414: 100%|██████████| 104/104 [00:01<00:00, 88.19it/s]\n",
            "epoch:87 loss:2.6637701988220215: 100%|██████████| 104/104 [00:01<00:00, 88.86it/s]\n",
            "epoch:88 loss:2.5886893272399902: 100%|██████████| 104/104 [00:01<00:00, 90.00it/s]\n",
            "epoch:89 loss:2.6950855255126953: 100%|██████████| 104/104 [00:01<00:00, 90.11it/s]\n",
            "epoch:90 loss:2.445709466934204: 100%|██████████| 104/104 [00:01<00:00, 89.58it/s]\n",
            "epoch:91 loss:2.265338659286499: 100%|██████████| 104/104 [00:01<00:00, 90.40it/s]\n",
            "epoch:92 loss:2.1685638427734375: 100%|██████████| 104/104 [00:01<00:00, 90.42it/s]\n",
            "epoch:93 loss:2.1544296741485596: 100%|██████████| 104/104 [00:01<00:00, 90.45it/s]\n",
            "epoch:94 loss:2.27101469039917: 100%|██████████| 104/104 [00:01<00:00, 90.91it/s]\n",
            "epoch:95 loss:2.059274435043335: 100%|██████████| 104/104 [00:01<00:00, 90.89it/s]\n",
            "epoch:96 loss:1.9092872142791748: 100%|██████████| 104/104 [00:01<00:00, 86.88it/s]\n",
            "epoch:97 loss:1.8968790769577026: 100%|██████████| 104/104 [00:01<00:00, 87.26it/s]\n",
            "epoch:98 loss:1.7995432615280151: 100%|██████████| 104/104 [00:01<00:00, 90.33it/s]\n",
            "epoch:99 loss:1.6893658638000488: 100%|██████████| 104/104 [00:01<00:00, 89.68it/s]\n",
            "epoch:100 loss:1.6043930053710938: 100%|██████████| 104/104 [00:01<00:00, 88.91it/s]\n",
            "epoch:101 loss:1.7303147315979004: 100%|██████████| 104/104 [00:01<00:00, 89.98it/s]\n",
            "epoch:102 loss:1.7190508842468262: 100%|██████████| 104/104 [00:01<00:00, 90.09it/s]\n",
            "epoch:103 loss:1.7249786853790283: 100%|██████████| 104/104 [00:01<00:00, 89.01it/s]\n",
            "epoch:104 loss:1.9310917854309082: 100%|██████████| 104/104 [00:01<00:00, 90.15it/s]\n",
            "epoch:105 loss:2.2691733837127686: 100%|██████████| 104/104 [00:01<00:00, 90.13it/s]\n",
            "epoch:106 loss:2.1048147678375244: 100%|██████████| 104/104 [00:01<00:00, 89.92it/s]\n",
            "epoch:107 loss:1.6784203052520752: 100%|██████████| 104/104 [00:01<00:00, 86.01it/s]\n",
            "epoch:108 loss:1.7470099925994873: 100%|██████████| 104/104 [00:01<00:00, 87.67it/s]\n",
            "epoch:109 loss:1.5578960180282593: 100%|██████████| 104/104 [00:01<00:00, 89.64it/s]\n",
            "epoch:110 loss:1.5675616264343262: 100%|██████████| 104/104 [00:01<00:00, 89.03it/s]\n",
            "epoch:111 loss:1.4663753509521484: 100%|██████████| 104/104 [00:01<00:00, 88.74it/s]\n",
            "epoch:112 loss:1.3359102010726929: 100%|██████████| 104/104 [00:01<00:00, 89.89it/s]\n",
            "epoch:113 loss:1.2708866596221924: 100%|██████████| 104/104 [00:01<00:00, 90.47it/s]\n",
            "epoch:114 loss:1.2671812772750854: 100%|██████████| 104/104 [00:01<00:00, 91.04it/s]\n",
            "epoch:115 loss:1.3698523044586182: 100%|██████████| 104/104 [00:01<00:00, 89.91it/s]\n",
            "epoch:116 loss:1.4486151933670044: 100%|██████████| 104/104 [00:01<00:00, 89.95it/s]\n",
            "epoch:117 loss:1.257269024848938: 100%|██████████| 104/104 [00:01<00:00, 86.38it/s]\n",
            "epoch:118 loss:1.2844996452331543: 100%|██████████| 104/104 [00:01<00:00, 88.13it/s]\n",
            "epoch:119 loss:1.1068142652511597: 100%|██████████| 104/104 [00:01<00:00, 90.30it/s]\n",
            "epoch:120 loss:1.007704257965088: 100%|██████████| 104/104 [00:01<00:00, 89.99it/s]\n",
            "epoch:121 loss:1.0970263481140137: 100%|██████████| 104/104 [00:01<00:00, 88.60it/s]\n",
            "epoch:122 loss:0.809346616268158: 100%|██████████| 104/104 [00:01<00:00, 89.57it/s]\n",
            "epoch:123 loss:0.8406243920326233: 100%|██████████| 104/104 [00:01<00:00, 90.15it/s]\n",
            "epoch:124 loss:0.8586282134056091: 100%|██████████| 104/104 [00:01<00:00, 90.03it/s]\n",
            "epoch:125 loss:0.8046808838844299: 100%|██████████| 104/104 [00:01<00:00, 90.69it/s]\n",
            "epoch:126 loss:1.1309868097305298: 100%|██████████| 104/104 [00:01<00:00, 90.29it/s]\n",
            "epoch:127 loss:2.1503560543060303: 100%|██████████| 104/104 [00:01<00:00, 88.99it/s]\n",
            "epoch:128 loss:3.0003092288970947: 100%|██████████| 104/104 [00:01<00:00, 86.54it/s]\n",
            "epoch:129 loss:1.9809993505477905: 100%|██████████| 104/104 [00:01<00:00, 89.07it/s]\n",
            "epoch:130 loss:1.371625542640686: 100%|██████████| 104/104 [00:01<00:00, 89.73it/s]\n",
            "epoch:131 loss:1.1406798362731934: 100%|██████████| 104/104 [00:01<00:00, 88.92it/s]\n",
            "epoch:132 loss:0.8824455738067627: 100%|██████████| 104/104 [00:01<00:00, 88.48it/s]\n",
            "epoch:133 loss:0.7025560736656189: 100%|██████████| 104/104 [00:01<00:00, 89.18it/s]\n",
            "epoch:134 loss:0.6294078230857849: 100%|██████████| 104/104 [00:01<00:00, 89.58it/s]\n",
            "epoch:135 loss:0.6294795870780945: 100%|██████████| 104/104 [00:01<00:00, 90.11it/s]\n",
            "epoch:136 loss:0.6900327205657959: 100%|██████████| 104/104 [00:01<00:00, 89.94it/s]\n",
            "epoch:137 loss:0.5369066596031189: 100%|██████████| 104/104 [00:01<00:00, 89.27it/s]\n",
            "epoch:138 loss:0.48305341601371765: 100%|██████████| 104/104 [00:01<00:00, 86.40it/s]\n",
            "epoch:139 loss:0.5396490693092346: 100%|██████████| 104/104 [00:01<00:00, 86.82it/s]\n",
            "epoch:140 loss:0.5507730841636658: 100%|██████████| 104/104 [00:01<00:00, 89.82it/s]\n",
            "epoch:141 loss:0.533924400806427: 100%|██████████| 104/104 [00:01<00:00, 89.96it/s]\n",
            "epoch:142 loss:0.5657116174697876: 100%|██████████| 104/104 [00:01<00:00, 89.41it/s]\n",
            "epoch:143 loss:0.5034303665161133: 100%|██████████| 104/104 [00:01<00:00, 90.43it/s]\n",
            "epoch:144 loss:0.6679478287696838: 100%|██████████| 104/104 [00:01<00:00, 90.31it/s]\n",
            "epoch:145 loss:0.7839240431785583: 100%|██████████| 104/104 [00:01<00:00, 90.41it/s]\n",
            "epoch:146 loss:0.5994975566864014: 100%|██████████| 104/104 [00:01<00:00, 91.06it/s]\n",
            "epoch:147 loss:0.737992525100708: 100%|██████████| 104/104 [00:01<00:00, 90.40it/s]\n",
            "epoch:148 loss:0.6731603145599365: 100%|██████████| 104/104 [00:01<00:00, 89.43it/s]\n",
            "epoch:149 loss:0.6952157616615295: 100%|██████████| 104/104 [00:01<00:00, 87.67it/s]\n",
            "epoch:150 loss:1.146414041519165: 100%|██████████| 104/104 [00:01<00:00, 89.67it/s]\n",
            "epoch:151 loss:0.7854698300361633: 100%|██████████| 104/104 [00:01<00:00, 90.12it/s]\n",
            "epoch:152 loss:1.5621731281280518: 100%|██████████| 104/104 [00:01<00:00, 87.82it/s]\n",
            "epoch:153 loss:0.41001054644584656: 100%|██████████| 104/104 [00:01<00:00, 89.11it/s]\n",
            "epoch:154 loss:0.5341027975082397: 100%|██████████| 104/104 [00:01<00:00, 90.74it/s]\n",
            "epoch:155 loss:0.4116402566432953: 100%|██████████| 104/104 [00:01<00:00, 90.46it/s]\n",
            "epoch:156 loss:0.3871363401412964: 100%|██████████| 104/104 [00:01<00:00, 89.79it/s]\n",
            "epoch:157 loss:0.3959119915962219: 100%|██████████| 104/104 [00:01<00:00, 90.35it/s]\n",
            "epoch:158 loss:0.2796960473060608: 100%|██████████| 104/104 [00:01<00:00, 87.98it/s]\n",
            "epoch:159 loss:0.38833531737327576: 100%|██████████| 104/104 [00:01<00:00, 86.61it/s]\n",
            "epoch:160 loss:0.31156882643699646: 100%|██████████| 104/104 [00:01<00:00, 87.11it/s]\n",
            "epoch:161 loss:0.2772666811943054: 100%|██████████| 104/104 [00:01<00:00, 89.93it/s]\n",
            "epoch:162 loss:0.25416290760040283: 100%|██████████| 104/104 [00:01<00:00, 87.51it/s]\n",
            "epoch:163 loss:0.253226637840271: 100%|██████████| 104/104 [00:01<00:00, 87.84it/s]\n",
            "epoch:164 loss:0.2654040455818176: 100%|██████████| 104/104 [00:01<00:00, 89.22it/s]\n",
            "epoch:165 loss:0.2661932706832886: 100%|██████████| 104/104 [00:01<00:00, 89.46it/s]\n",
            "epoch:166 loss:0.2168077528476715: 100%|██████████| 104/104 [00:01<00:00, 89.91it/s]\n",
            "epoch:167 loss:0.7961214184761047: 100%|██████████| 104/104 [00:01<00:00, 89.11it/s]\n",
            "epoch:168 loss:0.26229265332221985: 100%|██████████| 104/104 [00:01<00:00, 89.49it/s]\n",
            "epoch:169 loss:0.2403063327074051: 100%|██████████| 104/104 [00:01<00:00, 88.11it/s]\n",
            "epoch:170 loss:0.29267624020576477: 100%|██████████| 104/104 [00:01<00:00, 87.97it/s]\n",
            "epoch:171 loss:0.48912736773490906: 100%|██████████| 104/104 [00:01<00:00, 89.58it/s]\n",
            "epoch:172 loss:0.506453275680542: 100%|██████████| 104/104 [00:01<00:00, 89.48it/s]\n",
            "epoch:173 loss:0.24819046258926392: 100%|██████████| 104/104 [00:01<00:00, 88.70it/s]\n",
            "epoch:174 loss:0.20583634078502655: 100%|██████████| 104/104 [00:01<00:00, 89.96it/s]\n",
            "epoch:175 loss:0.20833374559879303: 100%|██████████| 104/104 [00:01<00:00, 89.59it/s]\n",
            "epoch:176 loss:0.1984451711177826: 100%|██████████| 104/104 [00:01<00:00, 90.42it/s]\n",
            "epoch:177 loss:0.33702561259269714: 100%|██████████| 104/104 [00:01<00:00, 90.74it/s]\n",
            "epoch:178 loss:0.34514686465263367: 100%|██████████| 104/104 [00:01<00:00, 90.59it/s]\n",
            "epoch:179 loss:0.23530849814414978: 100%|██████████| 104/104 [00:01<00:00, 88.47it/s]\n",
            "epoch:180 loss:0.22612100839614868: 100%|██████████| 104/104 [00:01<00:00, 87.28it/s]\n",
            "epoch:181 loss:0.20174865424633026: 100%|██████████| 104/104 [00:01<00:00, 89.52it/s]\n",
            "epoch:182 loss:0.3682202100753784: 100%|██████████| 104/104 [00:01<00:00, 90.18it/s]\n",
            "epoch:183 loss:0.24801219999790192: 100%|██████████| 104/104 [00:01<00:00, 89.03it/s]\n",
            "epoch:184 loss:0.191374734044075: 100%|██████████| 104/104 [00:01<00:00, 88.69it/s]\n",
            "epoch:185 loss:0.12176825851202011: 100%|██████████| 104/104 [00:01<00:00, 90.44it/s]\n",
            "epoch:186 loss:0.5016817450523376: 100%|██████████| 104/104 [00:01<00:00, 89.78it/s]\n",
            "epoch:187 loss:0.18551139533519745: 100%|██████████| 104/104 [00:01<00:00, 89.41it/s]\n",
            "epoch:188 loss:0.2923588156700134: 100%|██████████| 104/104 [00:01<00:00, 90.33it/s]\n",
            "epoch:189 loss:1.5263686180114746: 100%|██████████| 104/104 [00:01<00:00, 89.33it/s]\n",
            "epoch:190 loss:0.2529376447200775: 100%|██████████| 104/104 [00:01<00:00, 87.45it/s]\n",
            "epoch:191 loss:0.17172852158546448: 100%|██████████| 104/104 [00:01<00:00, 87.46it/s]\n",
            "epoch:192 loss:0.17851121723651886: 100%|██████████| 104/104 [00:01<00:00, 89.71it/s]\n",
            "epoch:193 loss:0.30793821811676025: 100%|██████████| 104/104 [00:01<00:00, 89.08it/s]\n",
            "epoch:194 loss:0.20442427694797516: 100%|██████████| 104/104 [00:01<00:00, 89.12it/s]\n",
            "epoch:195 loss:0.23717698454856873: 100%|██████████| 104/104 [00:01<00:00, 90.41it/s]\n",
            "epoch:196 loss:0.27795544266700745: 100%|██████████| 104/104 [00:01<00:00, 90.24it/s]\n",
            "epoch:197 loss:0.47159671783447266: 100%|██████████| 104/104 [00:01<00:00, 89.72it/s]\n",
            "epoch:198 loss:0.3507334887981415: 100%|██████████| 104/104 [00:01<00:00, 90.05it/s]\n",
            "epoch:199 loss:0.24726592004299164: 100%|██████████| 104/104 [00:01<00:00, 89.68it/s]\n",
            "epoch:200 loss:0.1731935441493988: 100%|██████████| 104/104 [00:01<00:00, 88.05it/s]\n",
            "epoch:201 loss:0.4357207119464874: 100%|██████████| 104/104 [00:01<00:00, 86.50it/s]\n",
            "epoch:202 loss:0.2069743573665619: 100%|██████████| 104/104 [00:01<00:00, 89.79it/s]\n",
            "epoch:203 loss:0.29990699887275696: 100%|██████████| 104/104 [00:01<00:00, 89.22it/s]\n",
            "epoch:204 loss:0.13099946081638336: 100%|██████████| 104/104 [00:01<00:00, 88.83it/s]\n",
            "epoch:205 loss:0.2874767780303955: 100%|██████████| 104/104 [00:01<00:00, 89.52it/s]\n",
            "epoch:206 loss:0.19225357472896576: 100%|██████████| 104/104 [00:01<00:00, 90.35it/s]\n",
            "epoch:207 loss:0.5182996988296509: 100%|██████████| 104/104 [00:01<00:00, 90.19it/s]\n",
            "epoch:208 loss:0.6009164452552795: 100%|██████████| 104/104 [00:01<00:00, 90.14it/s]\n",
            "epoch:209 loss:0.23614288866519928: 100%|██████████| 104/104 [00:01<00:00, 89.08it/s]\n",
            "epoch:210 loss:0.48753345012664795: 100%|██████████| 104/104 [00:01<00:00, 88.49it/s]\n",
            "epoch:211 loss:0.29943224787712097: 100%|██████████| 104/104 [00:01<00:00, 87.84it/s]\n",
            "epoch:212 loss:0.5146049857139587: 100%|██████████| 104/104 [00:01<00:00, 88.71it/s]\n",
            "epoch:213 loss:0.18327249586582184: 100%|██████████| 104/104 [00:01<00:00, 89.46it/s]\n",
            "epoch:214 loss:0.17333239316940308: 100%|██████████| 104/104 [00:01<00:00, 89.60it/s]\n",
            "epoch:215 loss:0.10495876520872116: 100%|██████████| 104/104 [00:01<00:00, 88.51it/s]\n",
            "epoch:216 loss:0.15848495066165924: 100%|██████████| 104/104 [00:01<00:00, 90.60it/s]\n",
            "epoch:217 loss:0.077446348965168: 100%|██████████| 104/104 [00:01<00:00, 90.67it/s]\n",
            "epoch:218 loss:0.12311534583568573: 100%|██████████| 104/104 [00:01<00:00, 90.22it/s]\n",
            "epoch:219 loss:0.06867530941963196: 100%|██████████| 104/104 [00:01<00:00, 90.34it/s]\n",
            "epoch:220 loss:0.11091726273298264: 100%|██████████| 104/104 [00:01<00:00, 89.90it/s]\n",
            "epoch:221 loss:0.061465345323085785: 100%|██████████| 104/104 [00:01<00:00, 87.94it/s]\n",
            "epoch:222 loss:0.4022397994995117: 100%|██████████| 104/104 [00:01<00:00, 86.46it/s]\n",
            "epoch:223 loss:0.35674425959587097: 100%|██████████| 104/104 [00:01<00:00, 89.95it/s]\n",
            "epoch:224 loss:0.10178267955780029: 100%|██████████| 104/104 [00:01<00:00, 90.17it/s]\n",
            "epoch:225 loss:0.1370004266500473: 100%|██████████| 104/104 [00:01<00:00, 89.76it/s]\n",
            "epoch:226 loss:0.10360440611839294: 100%|██████████| 104/104 [00:01<00:00, 89.61it/s]\n",
            "epoch:227 loss:0.1648254245519638: 100%|██████████| 104/104 [00:01<00:00, 89.69it/s]\n",
            "epoch:228 loss:0.07203057408332825: 100%|██████████| 104/104 [00:01<00:00, 89.55it/s]\n",
            "epoch:229 loss:0.21248678863048553: 100%|██████████| 104/104 [00:01<00:00, 90.00it/s]\n",
            "epoch:230 loss:0.06802064925432205: 100%|██████████| 104/104 [00:01<00:00, 89.65it/s]\n",
            "epoch:231 loss:0.21842828392982483: 100%|██████████| 104/104 [00:01<00:00, 87.72it/s]\n",
            "epoch:232 loss:0.06300293654203415: 100%|██████████| 104/104 [00:01<00:00, 84.93it/s]\n",
            "epoch:233 loss:0.06331531703472137: 100%|██████████| 104/104 [00:01<00:00, 86.47it/s]\n",
            "epoch:234 loss:0.1465139389038086: 100%|██████████| 104/104 [00:01<00:00, 87.25it/s]\n",
            "epoch:235 loss:0.1583331674337387: 100%|██████████| 104/104 [00:01<00:00, 89.49it/s]\n",
            "epoch:236 loss:0.11410105228424072: 100%|██████████| 104/104 [00:01<00:00, 89.62it/s]\n",
            "epoch:237 loss:0.06745002418756485: 100%|██████████| 104/104 [00:01<00:00, 89.78it/s]\n",
            "epoch:238 loss:0.24892301857471466: 100%|██████████| 104/104 [00:01<00:00, 89.65it/s]\n",
            "epoch:239 loss:0.06497950106859207: 100%|██████████| 104/104 [00:01<00:00, 89.17it/s]\n",
            "epoch:240 loss:0.12094779312610626: 100%|██████████| 104/104 [00:01<00:00, 89.08it/s]\n",
            "epoch:241 loss:0.07629653811454773: 100%|██████████| 104/104 [00:01<00:00, 88.18it/s]\n",
            "epoch:242 loss:0.11479893326759338: 100%|██████████| 104/104 [00:01<00:00, 86.03it/s]\n",
            "epoch:243 loss:0.05775932967662811: 100%|██████████| 104/104 [00:01<00:00, 86.94it/s]\n",
            "epoch:244 loss:0.0877775251865387: 100%|██████████| 104/104 [00:01<00:00, 89.87it/s]\n",
            "epoch:245 loss:0.03128742054104805: 100%|██████████| 104/104 [00:01<00:00, 89.59it/s]\n",
            "epoch:246 loss:0.056976187974214554: 100%|██████████| 104/104 [00:01<00:00, 89.24it/s]\n",
            "epoch:247 loss:0.09099666774272919: 100%|██████████| 104/104 [00:01<00:00, 89.98it/s]\n",
            "epoch:248 loss:0.043824389576911926: 100%|██████████| 104/104 [00:01<00:00, 90.01it/s]\n",
            "epoch:249 loss:0.07457104325294495: 100%|██████████| 104/104 [00:01<00:00, 90.05it/s]\n",
            "epoch:250 loss:0.050265345722436905: 100%|██████████| 104/104 [00:01<00:00, 89.22it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate(model, BOW, string=\"finding an \", strlen=10):\n",
        "  device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "  print(f\"input word: {string}\")\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for p in range(strlen):\n",
        "      words = torch.tensor(\n",
        "          [BOW[w] for w in string.split()], dtype=torch.long).to(device)\n",
        "\n",
        "\n",
        "      input_tensor = torch.unsqueeze(words[-2:], dim=0)\n",
        "      output = model(input_tensor)\n",
        "      output_word = (torch.argmax(output).cpu().numpy())\n",
        "      string += list(BOW.keys())[output_word]\n",
        "      string += \" \"\n",
        "\n",
        "    print(f'predicted sentence: {string}')\n",
        "\n",
        "\n",
        "model.load_state_dict(torch.load(\"lstm.pth\", map_location=device))\n",
        "pred= generate(model, dataset.BOW)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4cwzvwXFo2-D",
        "outputId": "77b621a4-6d33-43cc-fc4a-7e6a4433a738"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-11-62ee973ce888>:21: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load(\"lstm.pth\", map_location=device))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input word: finding an \n",
            "predicted sentence: finding an australia developer looks no legal to its answer’s exhusband epa \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 예시로 vocab_size=10000 이고 embedding_dim=16 일 때\n",
        "embedding_layer = nn.Embedding(num_embeddings=10000, embedding_dim=16)\n",
        "\n",
        "# 단어 인덱스 3을 임베딩 벡터로 변환\n",
        "word_index = torch.LongTensor([3])\n",
        "embedding_vector = embedding_layer(word_index)\n",
        "print(embedding_vector)  # 출력은 16차원의 밀집된 벡터\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Tie4_14GDm3",
        "outputId": "bf5bb114-f6fc-47a0-9b96-f9b80490e1f0"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[-0.8955, -0.8043, -0.6432,  0.5089, -1.5682, -1.6388, -0.8074,  0.6936,\n",
            "          0.9364,  0.2896,  0.8949, -1.0001,  0.1107, -1.2239, -1.5864, -0.5754]],\n",
            "       grad_fn=<EmbeddingBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 온도 샘플링 추가\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def generate(model, BOW, string=\"finding an \", strlen=10, temperature=0.5):\n",
        "    \"\"\"\n",
        "    시작 문장으로부터 모델을 통해 단어를 생성하여 문장을 예측합니다.\n",
        "\n",
        "    Args:\n",
        "        model (torch.nn.Module): 학습된 LSTM 모델.\n",
        "        BOW (dict): 단어와 인덱스의 매핑이 저장된 Bag of Words 딕셔너리.\n",
        "        string (str): 예측을 시작할 초기 단어들로 이루어진 문장.\n",
        "        strlen (int): 예측할 단어의 개수.\n",
        "        temperature (float): 샘플링 시 온도 값, 값이 높을수록 예측이 다양해집니다.\n",
        "\n",
        "    Returns:\n",
        "        str: 예측된 문장을 반환합니다.\n",
        "    \"\"\"\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "    # 시작 문장을 출력\n",
        "    print(f\"Input word: {string}\")\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for p in range(strlen):\n",
        "            # 현재 문장의 각 단어를 BOW 인덱스로 변환하여 텐서로 만듦\n",
        "            try:\n",
        "                words = torch.tensor([BOW[w] for w in string.split()], dtype=torch.long).to(device)\n",
        "            except KeyError as e:\n",
        "                print(f\"KeyError: 단어 '{e}'가 BOW에 없습니다.\")\n",
        "                return\n",
        "\n",
        "            # LSTM 입력을 위해 최근 두 단어만 선택하여 차원 추가\n",
        "            input_tensor = torch.unsqueeze(words[-2:], dim=0)\n",
        "\n",
        "            # 모델에 입력 텐서를 전달하여 예측 결과 생성\n",
        "            output = model(input_tensor)\n",
        "\n",
        "            # 온도 조정하여 소프트맥스를 적용하여 확률 분포 생성\n",
        "            output = output / temperature\n",
        "            probabilities = F.softmax(output, dim=-1).squeeze()\n",
        "\n",
        "            # 확률 분포에서 랜덤 샘플링하여 단어 선택\n",
        "            output_word = torch.multinomial(probabilities, 1).item()\n",
        "\n",
        "            # 예측된 단어를 BOW에서 찾아 문자열에 추가\n",
        "            string += list(BOW.keys())[output_word] + \" \"\n",
        "\n",
        "    # 최종 예측된 문장을 출력\n",
        "    print(f'Predicted sentence: {string.strip()}')\n",
        "    return string.strip()\n",
        "\n",
        "# 함수 호출\n",
        "pred = generate(model, dataset.BOW)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PCuhul6rM4bG",
        "outputId": "0146657a-9ff4-46b5-8b4d-6c4a67cad9d8"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input word: finding an \n",
            "Predicted sentence: finding an australia developer looks no legal a who biden in explores\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fLCvfqqGP3pD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
